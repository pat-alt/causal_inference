{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.1-cp38-cp38-macosx_10_13_x86_64.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 5.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/simonneumeyer/opt/anaconda3/envs/Desktop/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.0.0-py3-none-any.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /Users/simonneumeyer/opt/anaconda3/envs/Desktop/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.6.0)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=82c39fb16a67d49b55c7b29e6cd9d1fe96e914143ce9e06d7cfa0992d7a96015\n",
      "  Stored in directory: /Users/simonneumeyer/Library/Caches/pip/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.0.0 scikit-learn-0.24.1 sklearn-0.0 threadpoolctl-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load each comparison group dataset as a different dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cps_controls = pd.read_stata('../dw_data/cps_controls.dta')\n",
    "cps_controls2 = pd.read_stata('../dw_data/cps_controls2.dta')\n",
    "cps_controls3 = pd.read_stata('../dw_data/cps_controls3.dta')\n",
    "psid_controls = pd.read_stata('../dw_data/psid_controls.dta')\n",
    "psid_controls2 = pd.read_stata('../dw_data/psid_controls2.dta')\n",
    "psid_controls3 = pd.read_stata('../dw_data/psid_controls3.dta')\n",
    "nsw_dw = pd.read_stata('../dw_data/nsw_dw.dta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    260\n",
       "1.0    185\n",
       "Name: treat, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upon inspection we see that the experimental dataset (nsw_dw.dta) includes both treated and controlled individuals:\n",
    "nsw_dw.treat.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating and defining covariates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function that concatenates treatment and control data and adds all additional variables \n",
    "# as indicated in the footnote of Table 3 (Deheija & Wahba, 1999) to a dataset. \n",
    "# Those that are not needed are gonna be selected out later.\n",
    "\n",
    "def add_variables(df):\n",
    "    # concatenating treatment and control:\n",
    "    control_dataset = pd.concat([nsw_dw, df], axis=0)\n",
    "    control_dataset = control_dataset.reset_index(drop=True)\n",
    "\n",
    "    # unemployment rate dummies:\n",
    "    control_dataset['u74'] = control_dataset.re74.apply(lambda x: 1 if x == 0 else 0)\n",
    "    control_dataset['u75'] = control_dataset.re75.apply(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "    # higher order terms:\n",
    "    control_dataset['education_2'] = control_dataset.education**2\n",
    "    control_dataset['age_2'] = control_dataset.age**2\n",
    "    control_dataset['age_3'] = control_dataset.age**3\n",
    "    control_dataset['re74_2'] = control_dataset.re74**2\n",
    "    control_dataset['re75_2'] = control_dataset.re75**2\n",
    "\n",
    "    # interactions:\n",
    "    control_dataset['education_re74'] = control_dataset.education*control_dataset.re74\n",
    "    control_dataset['u74_black'] = control_dataset.u74*control_dataset.black\n",
    "    \n",
    "    return control_dataset\n",
    "\n",
    "# applying the function to each comparison group separately:\n",
    "cps_controls = add_variables(cps_controls)\n",
    "cps_controls2 = add_variables(cps_controls2)\n",
    "cps_controls3 = add_variables(cps_controls3)\n",
    "psid_controls = add_variables(psid_controls)\n",
    "psid_controls2 = add_variables(psid_controls2)\n",
    "psid_controls3 = add_variables(psid_controls3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# giving names to the datasets that will later help us use different covariates for each dataset:\n",
    "\n",
    "cps_controls.name = 'cps1'\n",
    "cps_controls2.name = 'cps2'\n",
    "cps_controls3.name = 'cps3'\n",
    "psid_controls.name = 'psid1'\n",
    "psid_controls2.name = 'psid2'\n",
    "psid_controls3.name = 'psid3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute propensity scores we define usage of covariates for different datasets:\n",
    "\n",
    "covariates_cps = ['age', 'age_2', \n",
    "              'education', 'education_2', 'nodegree', \n",
    "              'married', 'black', 'hispanic', 're74', 're75',\n",
    "              'u74', 'u75', 'education_re74', 'age_3']\n",
    "\n",
    "covariates_psd1 = ['age', 'age_2', \n",
    "              'education', 'education_2', 'nodegree', \n",
    "              'married', 'black', 'hispanic', 're74', 're75', 're74_2', 're75_2',\n",
    "              'u74_black']\n",
    "\n",
    "covariates_psd23 = ['age', 'age_2', \n",
    "              'education', 'education_2', 'nodegree', \n",
    "              'married', 'black', 'hispanic', 're74', 're75', 're74_2', 're75_2',\n",
    "              'u74', 'u75']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logistic regression to estimate propensity scores & nearest neighbor matching to obtain the ATT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATT for psid1: 887.01953125\n",
      "ATT for psid2: 1394.91748046875\n",
      "ATT for psid3: 2596.96435546875\n",
      "ATT for cps1: 927.732421875\n",
      "ATT for cps2: -388.21875\n",
      "ATT for cps3: 551.1279296875\n"
     ]
    }
   ],
   "source": [
    "# This function takes as an input a treated and a non-treated dataframe and creates \n",
    "# the counterfactual outcome of the treated units (Y0i) by matching (Nearest Neighbor) on propensity scores:\n",
    "def get_matching_pairs(treated_df, non_treated_df):\n",
    "\n",
    "    treated_x = treated_df.pscore.values\n",
    "    non_treated_x = non_treated_df.pscore.values\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=1).fit(non_treated_x.reshape(-1, 1))\n",
    "    distances, indices = nbrs.kneighbors(treated_x.reshape(-1, 1))\n",
    "    indices = indices.reshape(indices.shape[0])\n",
    "    matched = non_treated_df.loc[indices]\n",
    "    return matched\n",
    "\n",
    "\n",
    "#### logistic regression to estimate propensity scores: ####\n",
    "\n",
    "# looping over each comparison dataset:\n",
    "for control_dataset in [psid_controls, psid_controls2, psid_controls3, cps_controls, cps_controls2, cps_controls3]: \n",
    "    \n",
    "    # use different covariates for each control group:\n",
    "    if control_dataset.name == psid_controls.name:\n",
    "        covariates = covariates_psd1\n",
    "    elif control_dataset.name in [psid_controls2.name, psid_controls3.name]:\n",
    "        covariates = covariates_psd23\n",
    "    elif control_dataset.name in [cps_controls.name, cps_controls2.name, cps_controls3.name]:\n",
    "        covariates = covariates_cps\n",
    "        \n",
    "    # define dependent and independent variables:\n",
    "    y = control_dataset['treat']\n",
    "    X = control_dataset[covariates]\n",
    "    \n",
    "    # option to drop nodegree as indicated in the problem set:\n",
    "    control_dataset.drop('nodegree', axis=1, inplace=True)\n",
    "\n",
    "    # create propensity scores with a logit model:\n",
    "    logit = LogisticRegression(random_state=5, max_iter=1000)\n",
    "    model = logit.fit(X, y)\n",
    "    propensity = [n[1] for n in model.predict_proba(X)]\n",
    "    control_dataset['pscore'] = propensity\n",
    "    \n",
    "\n",
    "#### perform nearest neighbor matching based on propensity scores: ####\n",
    "\n",
    "    # The experimental controls are assigned to treatment as they are from the same distribution as treated\n",
    "    # (Deheija & Wahba p. 1057 bottom left)\n",
    "    treated_df = control_dataset[control_dataset.data_id=='Dehejia-Wahba Sample'].reset_index(drop=True)\n",
    "    non_treated_df = control_dataset[control_dataset.data_id!='Dehejia-Wahba Sample'].reset_index(drop=True)\n",
    "        \n",
    "    # apply nearest neighbor matching function defined above:\n",
    "    matched_df = get_matching_pairs(treated_df, non_treated_df)\n",
    "    # assign propensity scores to the dataframe of the treated:\n",
    "    treated_df['re78_cf'] = matched_df.re78.values\n",
    "    \n",
    "    # option to test whether pscore computation makes sense:\n",
    "    #print(matched_df.pscore.values - treated_df.pscore.values < 0.05)\n",
    "    \n",
    "    #### compute att: ####\n",
    "    # we select only the target group of interest (treated individuals of the experimental group):\n",
    "    att = treated_df[treated_df['treat']==1]['re78'].mean() - treated_df[treated_df['treat']==1]['re78_cf'].mean()\n",
    "    \n",
    "    print(f'ATT for {control_dataset.name}: {att}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = treated_df['pscore'].values\n",
    "#y = treated_df['re78'].values\n",
    "#X = sm.add_constant(X.ravel())\n",
    "#\n",
    "#results = sm.OLS(y,X).fit()\n",
    "#print(f'coefficient: {results.params[0]}')\n",
    "#print(f'standard error: {results.bse[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
